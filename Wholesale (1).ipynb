{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
      "0        2       3  12669  9656     7561     214              2674        1338\n",
      "1        2       3   7057  9810     9568    1762              3293        1776\n",
      "2        2       3   6353  8808     7684    2405              3516        7844\n",
      "3        1       3  13265  1196     4221    6404               507        1788\n",
      "4        2       3  22615  5410     7198    3915              1777        5185\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Wholesaledata.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Delicassen</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frozen</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grocery</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Milk</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fresh</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Channel</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total  Percent\n",
       "Delicassen            0      0.0\n",
       "Detergents_Paper      0      0.0\n",
       "Frozen                0      0.0\n",
       "Grocery               0      0.0\n",
       "Milk                  0      0.0\n",
       "Fresh                 0      0.0\n",
       "Region                0      0.0\n",
       "Channel               0      0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=[\"Total\", \"Percent\"])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel:  [2 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Channel: \",data.Channel.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region:  [3 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Region: \",data.Region.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # drop last column (extra column added by pd)\n",
    "    # and unnecessary first column (id)\n",
    "data.drop(data.columns[[0]], axis=1, inplace=True)\n",
    "    \n",
    "    #Mapping values of target variable quality to 'low', 'medium' and 'high' categories for classification\n",
    "data['Region']=data['Region'].map({1:'low',2:'medium',3:'high'})  \n",
    "    #df['quality']=df['quality'].map({'low':0,'medium':1,'high':2})\n",
    "    \n",
    "    \n",
    "diag_map = {'low': 1.0, 'medium': -1.0,'high':-1.0}\n",
    "data['Region'] = data['Region'].map(diag_map)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # put features & outputs in different data frames\n",
    "Y = data.loc[:, 'Region']\n",
    "X = data.iloc[:, 0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations_with_replacement\n",
    "from sympy.core import Mul\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Multiply(a):\n",
    "    ans = 1\n",
    "    for i in a:\n",
    "        ans*=i\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_features(variables):\n",
    "    ans = []\n",
    "    for i in range(1,5):\n",
    "        l = list(combinations_with_replacement(variables, i))\n",
    "        for j in l:\n",
    "            ans.append(Multiply(j))\n",
    "            #print(Multiply(j))\n",
    "    ans.append(1)\n",
    "    ans = np.array(ans).astype(float)\n",
    "    return ans.reshape((ans.shape[0],1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smo(X,Y, C=0.1, toler=0.001, maxIter=5):\n",
    "    #dataMatrix = mat(dataMatIn); \n",
    "    #labelMat = mat(classLabels).transpose()\n",
    "    b = 0; m = X.shape[0]\n",
    "    alpha=np.zeros(shape=(X.shape[0],1))\n",
    "    #alphas = mat(zeros((m,1)))\n",
    "    iter = 0\n",
    "    while (iter < maxIter):\n",
    "        alphaPairsChanged = 0\n",
    "        for i in range(m): #Fetch one piece of data each time, corresponding to alpha1\n",
    "            fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b #Calculate the classification function result, directly apply the classification function formula (8)\n",
    "            Ei = fXi-float(labelMat[i])#if checks if an example violates KKT conditions #The difference between the calculated result and the actual classification\n",
    "            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n",
    "                               #3 cabinets that do not meet the KKT condition in the iteration are used here, ((labelMat[i]*Ei> toler) and (alphas[i]> 0) as an example, assuming this is a positive classification, then (labelMat[i ]*Ei> toler)\n",
    "                               # Ei>0 and Ei> toler, because labelMat[i]=1, yi*fXi>1 is obtained, at this time, alpha1>0 is not satisfied, and the original alpha1=0\n",
    "                j = selectJrand(i,m)\n",
    "                fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b # alpha2 random selection\n",
    "                Ej = fXj - float(labelMat[j])\n",
    "                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\n",
    "                if (labelMat[i] != labelMat[j]): # y1 and y1 have different signs, calculate L and H\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                if L==H: \n",
    "                    print (\"L==H\") \n",
    "                    continue\n",
    "                    eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T-dataMatrix[i,:]*dataMatrix[i,:].T-dataMatrix[j,:]*dataMatrix[j,:]. T #Calculate η\n",
    "                if eta >= 0: \n",
    "                    print (\"eta>=0\") \n",
    "                    continue #When calculating η =0, divide by 0, skip this case temporarily\n",
    "                    alphas[j] -= labelMat[j]*(Ei-Ej)/eta #Calculate alpha2, refer to formula (5)\n",
    "                    alphas[j] = clipAlpha(alphas[j],H,L)\n",
    "                if (abs(alphas[j]-alphaJold) <0.00001):\n",
    "                    print (\"j not moving enough\")\n",
    "                    continue #reach the optimization threshold\n",
    "                    alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])#update i by the same amount as j\n",
    "                                                                                                                                                 #the update is in the oppostie direction Update alpha1, formula (6)\n",
    "                                 #Calculate b1,b2, formula (7)                                                        \n",
    "                    b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n",
    "                    b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                    if (0 < alphas[i]) and (C > alphas[i]):\n",
    "                        b = b1\n",
    "                    elif (0 < alphas[j]) and (C > alphas[j]): \n",
    "                        b = b2\n",
    "                    else: \n",
    "                        b = (b1 + b2)/2.0\n",
    "                        alphaPairsChanged += 1\n",
    "                        print (\"iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged))\n",
    "                if (alphaPairsChanged == 0): \n",
    "                    iter += 1\n",
    "                else: \n",
    "                    iter = 0\n",
    "        print (\"iteration number: %d\" % iter)\n",
    "         \n",
    "        return b,alphas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Y,alpha,b,x,sigma):\n",
    "    result=0.0\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        result+=(alpha[i]*Y[i]*(X[i,:] , x,sigma));\n",
    "\n",
    "    result+=b\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(W, X, Y):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge_loss = regularization_strength * (np.sum(distances) / N)\n",
    "\n",
    "    # calculate cost\n",
    "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(X):\n",
    "    corr_threshold = 0.9\n",
    "    corr = X.corr()\n",
    "    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= corr_threshold:\n",
    "                drop_columns[j] = True\n",
    "    columns_dropped = X.columns[drop_columns]\n",
    "    X.drop(columns_dropped, axis=1, inplace=True)\n",
    "    return columns_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeW(alphas, X, Y):\n",
    "    m,n = shape(X)\n",
    "    w = zeros((n,1))\n",
    "    for i in range(m):\n",
    "        w += multiply(alphas[i]*Y[i],X[i,:].T)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "    # if only one example is passed (eg. in case of SGD)\n",
    "    if type(Y_batch) == np.float64:\n",
    "        Y_batch = np.array([Y_batch])\n",
    "        X_batch = np.array([X_batch])  # gives multidimensional array\n",
    "\n",
    "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "    dw = np.zeros(len(W))\n",
    "    \n",
    "    for ind, d in enumerate(distance):\n",
    "        if max(0, d) == 0:\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (regularization_strength * Y_batch[ind] * X_batch[ind])\n",
    "        dw += di\n",
    "\n",
    "    dw = dw/len(Y_batch)  # average\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(features, outputs):\n",
    "    max_epochs = 5000\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    nth = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    cost_threshold = 0.01  # in percent\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "\n",
    "        # convergence check on 2^nth epoch\n",
    "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "            cost = compute_cost(weights, features, outputs)\n",
    "            print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n",
    "            # stoppage criterion\n",
    "            \n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init3():\n",
    "    print(\"reading dataset...\")\n",
    "    # read data in pandas (pd) data frame\n",
    "    data = pd.read_csv('Wholesaledata.csv')\n",
    "    print(data.head())\n",
    "\n",
    "    # drop last column (extra column added by pd)\n",
    "    # and unnecessary first column (id)\n",
    "    data.drop(data.columns[[0]], axis=1, inplace=True)\n",
    "    \n",
    "    #Mapping values of target variable quality to 'low', 'medium' and 'high' categories for classification\n",
    "    data['Region']=data['Region'].map({1:'low',2:'medium',3:'high'})  \n",
    "    #df['quality']=df['quality'].map({'low':0,'medium':1,'high':2})\n",
    "    \n",
    "    \n",
    "    diag_map = {'low': 1.0, 'medium': -1.0,'high':-1.0}\n",
    "    data['Region'] = data['Region'].map(diag_map)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # put features & outputs in different data frames\n",
    "    Y = data.loc[:, 'Region']\n",
    "    X = data.iloc[:, 0:-1]\n",
    "    # filter features\n",
    "    #remove_correlated_features(X)\n",
    "    #Xnew = np.apply_along_axis(new_features, 1, X)\n",
    "    #remove_less_significant_features(X, Y)\n",
    "    # normalize data for better convergence and to prevent overflow\n",
    "    #X_normalized = MinMaxScaler().fit_transform(Xnew)\n",
    "    #X = pd.DataFrame(X_normalized)\n",
    "\n",
    "    # insert 1 in every row for intercept b\n",
    "    #X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "\n",
    "    # split data into train and test set\n",
    "    print(\"splitting dataset into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)\n",
    "    # train the model\n",
    "    print(\"training started...\")\n",
    "    W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(\"training finished.\")\n",
    "    print(\"weights are: {}\".format(W))\n",
    "    print(W.shape)\n",
    "    # testing the model\n",
    "    print(\"testing the model...\")\n",
    "    y_train_predicted = np.array([])\n",
    "    for i in range(X_train.shape[0]):\n",
    "        yp = np.sign(np.dot(X_train.to_numpy()[i], W))\n",
    "        y_train_predicted = np.append(y_train_predicted, yp)\n",
    "\n",
    "    y_test_predicted = np.array([])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        yp = np.sign(np.dot(X_test.to_numpy()[i], W))\n",
    "        y_test_predicted = np.append(y_test_predicted, yp)\n",
    "    # An unknow data given by me to check which class it belongs to\n",
    "    #a = np.array([0.8, 0.6, 0.75, 0.75, 1])\n",
    "    #yp = np.sign(np.dot(a, W))\n",
    "    #print(\"Y value for a : \",yp)\n",
    "    print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "    print(\"recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n",
    "    print(\"precision on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_strength = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading dataset...\n",
      "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
      "0        2       3  12669  9656     7561     214              2674        1338\n",
      "1        2       3   7057  9810     9568    1762              3293        1776\n",
      "2        2       3   6353  8808     7684    2405              3516        7844\n",
      "3        1       3  13265  1196     4221    6404               507        1788\n",
      "4        2       3  22615  5410     7198    3915              1777        5185\n",
      "splitting dataset into train and test sets...\n",
      "training started...\n",
      "Epoch is: 1 and Cost is: 29815034026.192158\n",
      "Epoch is: 2 and Cost is: 29926506943.72587\n",
      "Epoch is: 4 and Cost is: 68243382974.069305\n",
      "Epoch is: 8 and Cost is: 28205400052.650566\n",
      "Epoch is: 16 and Cost is: 32540448512.04837\n",
      "Epoch is: 32 and Cost is: 117869379986.00006\n",
      "Epoch is: 64 and Cost is: 86056396183.03453\n",
      "Epoch is: 128 and Cost is: 20092450820.406536\n",
      "Epoch is: 256 and Cost is: 157181661057.41254\n",
      "Epoch is: 512 and Cost is: 47943140835.52242\n",
      "Epoch is: 1024 and Cost is: 37507856787.57003\n",
      "Epoch is: 2048 and Cost is: 119573198625.78006\n",
      "Epoch is: 4096 and Cost is: 44702954007.11338\n",
      "Epoch is: 4999 and Cost is: 54458238830.798676\n",
      "training finished.\n",
      "weights are: [   310.03335539 -14388.38857504    544.45967523 -12643.8241153\n",
      "  40131.67165373   -893.73241681]\n",
      "(6,)\n",
      "testing the model...\n",
      "accuracy on test dataset: 0.7840909090909091\n",
      "recall on test dataset: 0.3333333333333333\n",
      "precision on test dataset: 0.3333333333333333\n",
      "TIME TAKEN: 25.960947513580322 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "init3()\n",
    "end = time.time()\n",
    "print(\"TIME TAKEN: {} \\n\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Wholesaledata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29703</td>\n",
       "      <td>12051</td>\n",
       "      <td>16027</td>\n",
       "      <td>13135</td>\n",
       "      <td>182</td>\n",
       "      <td>2204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>39228</td>\n",
       "      <td>1431</td>\n",
       "      <td>764</td>\n",
       "      <td>4510</td>\n",
       "      <td>93</td>\n",
       "      <td>2346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14531</td>\n",
       "      <td>15488</td>\n",
       "      <td>30243</td>\n",
       "      <td>437</td>\n",
       "      <td>14841</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10290</td>\n",
       "      <td>1981</td>\n",
       "      <td>2232</td>\n",
       "      <td>1038</td>\n",
       "      <td>168</td>\n",
       "      <td>2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2787</td>\n",
       "      <td>1698</td>\n",
       "      <td>2510</td>\n",
       "      <td>65</td>\n",
       "      <td>477</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Channel  Region  Fresh   Milk  Grocery  Frozen  Detergents_Paper  \\\n",
       "0          2       3  12669   9656     7561     214              2674   \n",
       "1          2       3   7057   9810     9568    1762              3293   \n",
       "2          2       3   6353   8808     7684    2405              3516   \n",
       "3          1       3  13265   1196     4221    6404               507   \n",
       "4          2       3  22615   5410     7198    3915              1777   \n",
       "..       ...     ...    ...    ...      ...     ...               ...   \n",
       "435        1       3  29703  12051    16027   13135               182   \n",
       "436        1       3  39228   1431      764    4510                93   \n",
       "437        2       3  14531  15488    30243     437             14841   \n",
       "438        1       3  10290   1981     2232    1038               168   \n",
       "439        1       3   2787   1698     2510      65               477   \n",
       "\n",
       "     Delicassen  \n",
       "0          1338  \n",
       "1          1776  \n",
       "2          7844  \n",
       "3          1788  \n",
       "4          5185  \n",
       "..          ...  \n",
       "435        2204  \n",
       "436        2346  \n",
       "437        1867  \n",
       "438        2125  \n",
       "439          52  \n",
       "\n",
       "[440 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lable1 = df_train[\"Region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3\n",
       "1      3\n",
       "2      3\n",
       "3      3\n",
       "4      3\n",
       "      ..\n",
       "435    3\n",
       "436    3\n",
       "437    3\n",
       "438    3\n",
       "439    3\n",
       "Name: Region, Length: 440, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lable1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('Region',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1</td>\n",
       "      <td>29703</td>\n",
       "      <td>12051</td>\n",
       "      <td>16027</td>\n",
       "      <td>13135</td>\n",
       "      <td>182</td>\n",
       "      <td>2204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>1</td>\n",
       "      <td>39228</td>\n",
       "      <td>1431</td>\n",
       "      <td>764</td>\n",
       "      <td>4510</td>\n",
       "      <td>93</td>\n",
       "      <td>2346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2</td>\n",
       "      <td>14531</td>\n",
       "      <td>15488</td>\n",
       "      <td>30243</td>\n",
       "      <td>437</td>\n",
       "      <td>14841</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1</td>\n",
       "      <td>10290</td>\n",
       "      <td>1981</td>\n",
       "      <td>2232</td>\n",
       "      <td>1038</td>\n",
       "      <td>168</td>\n",
       "      <td>2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1</td>\n",
       "      <td>2787</td>\n",
       "      <td>1698</td>\n",
       "      <td>2510</td>\n",
       "      <td>65</td>\n",
       "      <td>477</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Channel  Fresh   Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0          2  12669   9656     7561     214              2674        1338\n",
       "1          2   7057   9810     9568    1762              3293        1776\n",
       "2          2   6353   8808     7684    2405              3516        7844\n",
       "3          1  13265   1196     4221    6404               507        1788\n",
       "4          2  22615   5410     7198    3915              1777        5185\n",
       "..       ...    ...    ...      ...     ...               ...         ...\n",
       "435        1  29703  12051    16027   13135               182        2204\n",
       "436        1  39228   1431      764    4510                93        2346\n",
       "437        2  14531  15488    30243     437             14841        1867\n",
       "438        1  10290   1981     2232    1038               168        2125\n",
       "439        1   2787   1698     2510      65               477          52\n",
       "\n",
       "[440 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train,df_lable1,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel = 'rbf')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "TIME TAKEN: 0.001993894577026367 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "print(model.score(X_test,y_test))\n",
    "end = time.time()\n",
    "print(\"TIME TAKEN: {} \\n\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lvec(a):\n",
    "    n = a.shape[0]\n",
    "    s = np.empty(0)\n",
    "    for i in range(n):\n",
    "        temp = a[i]*a[i:]\n",
    "        temp[0] = temp[0]/2\n",
    "        s = np.append(s,temp)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_z(x):\n",
    "    s = lvec(x)\n",
    "    z = np.empty(0)\n",
    "    z = np.append(z,s)\n",
    "    z = np.append(z,x)\n",
    "    z = np.append(z,1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_r(x):\n",
    "    z = create_z(x)\n",
    "    eta = lvec(z)\n",
    "    s = lvec(x)\n",
    "    r = np.empty(0)\n",
    "    r = np.append(r,eta)\n",
    "    r = np.append(r,s)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feature(x):\n",
    "    r = create_r(x)\n",
    "    xnew = np.empty(0)\n",
    "    xnew = np.append(xnew,r)\n",
    "    xnew = np.append(xnew,x)\n",
    "    xnew = np.append(xnew,1)\n",
    "    return xnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feature(x):\n",
    "    r = create_r(x)\n",
    "    xnew = np.empty(0)\n",
    "    xnew = np.append(xnew,r)\n",
    "    xnew = np.append(xnew,x)\n",
    "    xnew = np.append(xnew,1)\n",
    "    return xnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(X):\n",
    "    corr_threshold = 0.9\n",
    "    corr = X.corr()\n",
    "    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= corr_threshold:\n",
    "                drop_columns[j] = True\n",
    "    columns_dropped = X.columns[drop_columns]\n",
    "    X.drop(columns_dropped, axis=1, inplace=True)\n",
    "    return columns_dropped\n",
    "\n",
    "\n",
    "def remove_less_significant_features(X, Y):\n",
    "    sl = 0.05\n",
    "    regression_ols = None\n",
    "    columns_dropped = np.array([])\n",
    "    for itr in range(0, len(X.columns)):\n",
    "        regression_ols = sm.OLS(Y, X).fit()\n",
    "        max_col = regression_ols.pvalues.idxmax()\n",
    "        max_val = regression_ols.pvalues.max()\n",
    "        if max_val > sl:\n",
    "            X.drop(max_col, axis='columns', inplace=True)\n",
    "            columns_dropped = np.append(columns_dropped, [max_col])\n",
    "        else:\n",
    "            break\n",
    "    regression_ols.summary()\n",
    "    return columns_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(W, X, Y):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge_loss = regularization_strength * (np.sum(distances) / N)\n",
    "\n",
    "    # calculate cost\n",
    "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "    # if only one example is passed (eg. in case of SGD)\n",
    "    if type(Y_batch) == np.float64:\n",
    "        Y_batch = np.array([Y_batch])\n",
    "        X_batch = np.array([X_batch])  # gives multidimensional array\n",
    "\n",
    "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "    dw = np.zeros(len(W))\n",
    "\n",
    "    for ind, d in enumerate(distance):\n",
    "        if max(0, d) == 0:\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (regularization_strength * Y_batch[ind] * X_batch[ind])\n",
    "        dw += di\n",
    "\n",
    "    dw = dw/len(Y_batch)  # average\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(features, outputs):\n",
    "    max_epochs = 5000\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    nth = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    cost_threshold = 0.01  # in percent\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "\n",
    "        # convergence check on 2^nth epoch\n",
    "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "            cost = compute_cost(weights, features, outputs)\n",
    "            print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n",
    "            # stoppage criterion\n",
    "            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "                return weights\n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init3():\n",
    "    print(\"reading dataset...\")\n",
    "    # read data in pandas (pd) data frame\n",
    "    data = pd.read_csv('Wholesaledata.csv')\n",
    "    print(data.head())\n",
    "\n",
    "    # drop last column (extra column added by pd)\n",
    "    # and unnecessary first column (id)\n",
    "    data.drop(data.columns[[0]], axis=1, inplace=True)\n",
    "    \n",
    "    #Mapping values of target variable quality to 'low', 'medium' and 'high' categories for classification\n",
    "    data['Region']=data['Region'].map({1:'low',2:'medium',3:'high'})  \n",
    "    #df['quality']=df['quality'].map({'low':0,'medium':1,'high':2})\n",
    "    \n",
    "    \n",
    "    diag_map = {'low': 1.0, 'medium': -1.0,'high':-1.0}\n",
    "    data['Region'] = data['Region'].map(diag_map)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # put features & outputs in different data frames\n",
    "    Y = data.loc[:, 'Region']\n",
    "    X = data.iloc[:, 0:-1]\n",
    "    # filter features\n",
    "    #remove_correlated_features(X)\n",
    "    #Xnew = np.apply_along_axis(new_features, 1, X)\n",
    "    #remove_less_significant_features(X, Y)\n",
    "    # normalize data for better convergence and to prevent overflow\n",
    "    #X_normalized = MinMaxScaler().fit_transform(Xnew)\n",
    "    #X = pd.DataFrame(X_normalized)\n",
    "\n",
    "    # insert 1 in every row for intercept b\n",
    "    #X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "\n",
    "    # split data into train and test set\n",
    "    print(\"splitting dataset into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)\n",
    "    # train the model\n",
    "    print(\"training started...\")\n",
    "    W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(\"training finished.\")\n",
    "    print(\"weights are: {}\".format(W))\n",
    "    print(W.shape)\n",
    "    # testing the model\n",
    "    print(\"testing the model...\")\n",
    "    y_train_predicted = np.array([])\n",
    "    for i in range(X_train.shape[0]):\n",
    "        yp = np.sign(np.dot(X_train.to_numpy()[i], W))\n",
    "        y_train_predicted = np.append(y_train_predicted, yp)\n",
    "\n",
    "    y_test_predicted = np.array([])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        yp = np.sign(np.dot(X_test.to_numpy()[i], W))\n",
    "        y_test_predicted = np.append(y_test_predicted, yp)\n",
    "    # An unknow data given by me to check which class it belongs to\n",
    "    #a = np.array([0.8, 0.6, 0.75, 0.75, 1])\n",
    "    #yp = np.sign(np.dot(a, W))\n",
    "    #print(\"Y value for a : \",yp)\n",
    "    print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "    print(\"recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n",
    "    print(\"precision on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_strength = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading dataset...\n",
      "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
      "0        2       3  12669  9656     7561     214              2674        1338\n",
      "1        2       3   7057  9810     9568    1762              3293        1776\n",
      "2        2       3   6353  8808     7684    2405              3516        7844\n",
      "3        1       3  13265  1196     4221    6404               507        1788\n",
      "4        2       3  22615  5410     7198    3915              1777        5185\n",
      "splitting dataset into train and test sets...\n",
      "training started...\n",
      "Epoch is: 1 and Cost is: 27506365599.993946\n",
      "Epoch is: 2 and Cost is: 36928072596.547455\n",
      "Epoch is: 4 and Cost is: 52024208353.970055\n",
      "Epoch is: 8 and Cost is: 247915089345.0775\n",
      "Epoch is: 16 and Cost is: 17342624572.93755\n",
      "Epoch is: 32 and Cost is: 105125012405.60649\n",
      "Epoch is: 64 and Cost is: 82133652228.47559\n",
      "Epoch is: 128 and Cost is: 51936016797.39778\n",
      "Epoch is: 256 and Cost is: 40059928047.72598\n",
      "Epoch is: 512 and Cost is: 111461035763.88658\n",
      "Epoch is: 1024 and Cost is: 83153822028.00027\n",
      "Epoch is: 2048 and Cost is: 578697054670.007\n",
      "Epoch is: 4096 and Cost is: 93292908516.40912\n",
      "Epoch is: 4999 and Cost is: 147947979200.3809\n",
      "training finished.\n",
      "weights are: [   303.19111948  -5469.03133009 -32744.1301812  -41411.60156172\n",
      " -51634.45043663 -11126.72856184]\n",
      "(6,)\n",
      "testing the model...\n",
      "accuracy on test dataset: 0.8977272727272727\n",
      "recall on test dataset: 0.0\n",
      "precision on test dataset: 0.0\n",
      "TIME TAKEN: 27.752055883407593 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "init3()\n",
    "end = time.time()\n",
    "print(\"TIME TAKEN: {} \\n\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
